{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, numpy as np\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pandas.read_csv('COMMENTSFILE.csv')\n",
    "comments.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = comments[['docket_id', 'subject', 'title', 'gov_agency', 'abstract']]\n",
    "c.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: each reg per agency, generate ldavis\n",
    "fs_df.docket_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = str(sent)\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = test_df.comment.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv \n",
    "import pandas as pd \n",
    "\n",
    "test_df = pd.read_csv('data/fs_comments.csv')\n",
    "test_df = test_df.sample(4000)\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "test_df[\"comment\"] = test_df['comment'].str.replace('[^\\w\\s]','')\n",
    "test_df['stemmed'] = test_df['comment'].apply(lambda x: stemmer.stem(str(x).lower())) \n",
    "test_df['stemmed'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(test_df['stemmed'])\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Build LDA Model with GridSearch params\n",
    "lda_model = LatentDirichletAllocation(n_components=50,\n",
    "                                      learning_decay=0.5,         \n",
    "                                      max_iter=50,                \n",
    "                                      learning_method='online',   \n",
    "                                      random_state=42,            \n",
    "                                      batch_size=5000,            \n",
    "                                      evaluate_every = -1,        \n",
    "                                      n_jobs = -1)                \n",
    "lda_output = lda_model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtm = cv.fit_transform(test_df['stemmed'])\n",
    "features = cv.transform(test_df['stemmed'])\n",
    "features = pd.DataFrame(\n",
    "    features.todense(), index=test_df.index\n",
    ")\n",
    "\n",
    "#print(type(features))\n",
    "\n",
    "doc_topics = lda_model.transform(features)\n",
    "topic_matrix = pd.DataFrame(\n",
    "    doc_topics,\n",
    "    #10 topics\n",
    "    columns=[\"topic_{}\".format(i) for i in range(0, 50)],\n",
    "    index=features.index,\n",
    ")\n",
    "\n",
    "topic_matrix.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix = pd.merge(topic_matrix, test_df[['stemmed', 'comment', 'docket_id']], left_index=True, right_index=True)\n",
    "full_matrix.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_matrix.to_csv('fs_new_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "def show_topics(vectorizer=cv, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=cv, lda_model=lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pandas.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_keywords.to_csv('fs_new_total_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,topic in enumerate(lda_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
